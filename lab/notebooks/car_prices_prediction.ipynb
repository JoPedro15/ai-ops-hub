{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from ai_utils_client.data_ingestor_client import DataIngestorClient\n",
    "\n",
    "# Automation Hub Client Imports\n",
    "from ai_utils_client.data_processor_client import DataProcessorClient\n",
    "from core_lib_client.logger_client import logger\n",
    "from dotenv import load_dotenv\n",
    "from gdrive_client import GDriveClient\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add automation-hub to sys.path to resolve the 'clients' module error\n",
    "# This ensures that 'from clients.gdrive' works inside the library\n",
    "hub_path = str(Path(os.getcwd()).parents[1] / \"automation-hub\")\n",
    "if hub_path not in sys.path:\n",
    "    sys.path.append(hub_path)\n",
    "\n",
    "# Load environment variables from ai-lab root\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration from Single Source of Truth (.env)\n",
    "GDRIVE_FILE_ID: str | None = os.getenv(\"CAR_DATA_FILE_ID\")\n",
    "# Local Directories\n",
    "RAW_DIR: str = os.getenv(\"LOCAL_RAW_DIR\", \"data/raw\")\n",
    "PROCESSED_DIR: str = os.getenv(\"LOCAL_PROCESSED_DIR\", \"data/processed\")\n",
    "MODELS_DIR: str = os.getenv(\"LOCAL_MODELS_DIR\", \"data/models\")\n",
    "\n",
    "# GDrive IDs\n",
    "GDRIVE_PROCESSED_DATA_ID: str | None = os.getenv(\"GDRIVE_DATA_PROCESSED_FOLDER_ID\")\n",
    "GDRIVE_MODELS_PROD_ID: str | None = os.getenv(\"GDRIVE_MODELS_PROD_FOLDER_ID\")\n",
    "GDRIVE_MODELS_DEV_ID: str | None = os.getenv(\"GDRIVE_MODELS_DEV_FOLDER_ID\")\n",
    "\n",
    "# Initialize Shared Automation Clients\n",
    "gdrive: GDriveClient = GDriveClient()\n",
    "data_processor: DataProcessorClient = DataProcessorClient()\n",
    "data_ingestor: DataIngestorClient = DataIngestorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(\n",
    "    data: pd.DataFrame, features: list[str]\n",
    ") -> tuple[Any, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Standardizes features and fits an Ordinary Least Squares (OLS) regression model.\n",
    "    \"\"\"\n",
    "    # Feature selection and target isolation\n",
    "    X: pd.DataFrame = data[features].copy()\n",
    "    y: pd.Series = data[\"Price\"]\n",
    "\n",
    "    # Feature scaling (Standardization)\n",
    "    scaler: StandardScaler = StandardScaler()\n",
    "    X_scaled: Any = scaler.fit_transform(X)\n",
    "\n",
    "    # Reconstruct DataFrame to maintain metadata for Statsmodels\n",
    "    X_scaled_df: pd.DataFrame = pd.DataFrame(X_scaled, columns=features, index=X.index)\n",
    "\n",
    "    # Add constant for Intercept (Alpha)\n",
    "    X_final: pd.DataFrame = sm.add_constant(X_scaled_df)\n",
    "\n",
    "    # Fit OLS Model\n",
    "    model: Any = sm.OLS(y, X_final).fit()\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_assets(\n",
    "    model: Any,\n",
    "    scaler: StandardScaler,\n",
    "    df_prepared: pd.DataFrame,\n",
    "    env: str = \"prod\",  # Added environment toggle\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Persists artifacts locally and syncs to specific GDrive folders.\n",
    "    \"\"\"\n",
    "    # 1. Local Persistence (Always ensure directories exist)\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "    csv_path: str = os.path.join(PROCESSED_DIR, \"df_prepared.csv\")\n",
    "    model_path: str = os.path.join(MODELS_DIR, \"car_price_model.pkl\")\n",
    "    scaler_path: str = os.path.join(MODELS_DIR, \"scaler.pkl\")\n",
    "\n",
    "    df_prepared.to_csv(csv_path, index=False)\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    # 2. GDrive Sync - Processed Data\n",
    "    if GDRIVE_PROCESSED_DATA_ID:\n",
    "        logger.info(f\"Syncing processed data to GDrive ID: {GDRIVE_PROCESSED_DATA_ID}\")\n",
    "        gdrive.upload_file(file_path=csv_path, folder_id=GDRIVE_PROCESSED_DATA_ID)\n",
    "\n",
    "    # 3. GDrive Sync - Models (Logic for Dev vs Prod)\n",
    "    # If we are just testing, we should use GDRIVE_MODELS_DEV_ID\n",
    "    target_folder: str | None = (\n",
    "        GDRIVE_MODELS_PROD_ID if env == \"prod\" else GDRIVE_MODELS_DEV_ID\n",
    "    )\n",
    "\n",
    "    if target_folder:\n",
    "        logger.info(f\"Syncing model artifacts to GDrive Folder: {target_folder}\")\n",
    "        gdrive.upload_file(file_path=model_path, folder_id=target_folder)\n",
    "        gdrive.upload_file(file_path=scaler_path, folder_id=target_folder)\n",
    "        logger.success(f\"Artifacts successfully synced to {env.upper()} environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment() -> None:\n",
    "    \"\"\"\n",
    "    Main pipeline execution: Ingestion, Engineering, Training, and Export.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.section(\"STARTING STANDARDIZED EXPERIMENT\")\n",
    "\n",
    "    # Ingestion Phase\n",
    "    local_raw_file_path: str = os.path.join(RAW_DIR, \"cars.xls\")\n",
    "    df_raw: pd.DataFrame = data_ingestor.get_spreadsheet_data(\n",
    "        file_id=GDRIVE_FILE_ID, local_file_path=local_raw_file_path\n",
    "    )\n",
    "\n",
    "    # Normalize columns\n",
    "    df_raw.columns = df_raw.columns.str.strip().str.capitalize()\n",
    "\n",
    "    # Categorical Encoding (Make, Type)\n",
    "    logger.info(\"Encoding categorical features...\")\n",
    "    categorical_cols: list[str] = [\"Make\", \"Model\", \"Type\"]\n",
    "    df_prepared: pd.DataFrame = data_processor.encode_categorical_features(\n",
    "        df=df_raw, columns=categorical_cols, drop_first=True\n",
    "    )\n",
    "\n",
    "    # Feature Selection\n",
    "    base_numerical: list[str] = [\"Mileage\", \"Doors\"]\n",
    "    binary_flags: list[str] = [\"Leather\"]\n",
    "    encoded_features: list[str] = [\n",
    "        col\n",
    "        for col in df_prepared.columns\n",
    "        if col.startswith((\"Make_\", \"Model_\", \"Type_\"))\n",
    "    ]\n",
    "\n",
    "    active_features: list[str] = base_numerical + binary_flags + encoded_features\n",
    "    logger.info(f\"Training with {len(active_features)} active features.\")\n",
    "\n",
    "    # Model Training\n",
    "    model, scaler = train_linear_model(df_prepared, active_features)\n",
    "\n",
    "    # Reporting\n",
    "    logger.section(\"REGRESSION SUMMARY REPORT\")\n",
    "    display(model.summary())\n",
    "\n",
    "    # 6. Persistence\n",
    "    logger.section(\"EXPORT ASSETS\")\n",
    "    export_assets(model, scaler, df_prepared, env=\"prod\")\n",
    "    logger.success(\"EXPERIMENT COMPLETED AND ARCHIVED\")\n",
    "\n",
    "\n",
    "# Execute the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
